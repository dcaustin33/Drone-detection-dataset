{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "file \"/Users/derek/Desktop/Drone-detection-dataset/groundingdino/config/GroundingDINO_SwinT_OGC.py\" does not exist",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mcv2\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mload_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroundingdino/config/GroundingDINO_SwinT_OGC.py\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgroundingdino_swint_ogc.pth\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m IMAGE_PATH \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/derek/Desktop/Drone-detection-dataset/output_frames/V_DRONE_001/frame_0000.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      7\u001b[0m TEXT_PROMPT \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mflying object\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[0;32m~/Desktop/GroundingDINO/groundingdino/util/inference.py:30\u001b[0m, in \u001b[0;36mload_model\u001b[0;34m(model_config_path, model_checkpoint_path, device)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_model\u001b[39m(model_config_path: \u001b[38;5;28mstr\u001b[39m, model_checkpoint_path: \u001b[38;5;28mstr\u001b[39m, device: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m---> 30\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[43mSLConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfromfile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_config_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m     args\u001b[38;5;241m.\u001b[39mdevice \u001b[38;5;241m=\u001b[39m device\n\u001b[1;32m     32\u001b[0m     model \u001b[38;5;241m=\u001b[39m build_model(args)\n",
      "File \u001b[0;32m~/Desktop/GroundingDINO/groundingdino/util/slconfig.py:185\u001b[0m, in \u001b[0;36mSLConfig.fromfile\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfromfile\u001b[39m(filename):\n\u001b[0;32m--> 185\u001b[0m     cfg_dict, cfg_text \u001b[38;5;241m=\u001b[39m \u001b[43mSLConfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_file2dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m SLConfig(cfg_dict, cfg_text\u001b[38;5;241m=\u001b[39mcfg_text, filename\u001b[38;5;241m=\u001b[39mfilename)\n",
      "File \u001b[0;32m~/Desktop/GroundingDINO/groundingdino/util/slconfig.py:79\u001b[0m, in \u001b[0;36mSLConfig._file2dict\u001b[0;34m(filename)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_file2dict\u001b[39m(filename):\n\u001b[1;32m     78\u001b[0m     filename \u001b[38;5;241m=\u001b[39m osp\u001b[38;5;241m.\u001b[39mabspath(osp\u001b[38;5;241m.\u001b[39mexpanduser(filename))\n\u001b[0;32m---> 79\u001b[0m     \u001b[43mcheck_file_exist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     80\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m filename\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.py\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m     81\u001b[0m         \u001b[38;5;28;01mwith\u001b[39;00m tempfile\u001b[38;5;241m.\u001b[39mTemporaryDirectory() \u001b[38;5;28;01mas\u001b[39;00m temp_config_dir:\n",
      "File \u001b[0;32m~/Desktop/GroundingDINO/groundingdino/util/slconfig.py:23\u001b[0m, in \u001b[0;36mcheck_file_exist\u001b[0;34m(filename, msg_tmpl)\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_file_exist\u001b[39m(filename, msg_tmpl\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfile \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m does not exist\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m osp\u001b[38;5;241m.\u001b[39misfile(filename):\n\u001b[0;32m---> 23\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(msg_tmpl\u001b[38;5;241m.\u001b[39mformat(filename))\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: file \"/Users/derek/Desktop/Drone-detection-dataset/groundingdino/config/GroundingDINO_SwinT_OGC.py\" does not exist"
     ]
    }
   ],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, batch_predict\n",
    "import cv2\n",
    "import torch\n",
    "\n",
    "model = load_model(\"groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\")\n",
    "IMAGE_PATH = \"/Users/derek/Desktop/Drone-detection-dataset/output_frames/V_DRONE_001/frame_0000.jpg\"\n",
    "TEXT_PROMPT = \"flying object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "\n",
    "image_source, image = load_image(IMAGE_PATH)\n",
    "images = torch.stack([image, image])\n",
    "boxes, logits, boxes_to_im = batch_predict(\n",
    "    model=model,\n",
    "    preprocessed_images=images,\n",
    "    caption=TEXT_PROMPT,\n",
    "    box_threshold=BOX_TRESHOLD,\n",
    "    text_threshold=TEXT_TRESHOLD,\n",
    "    device=\"cpu\"\n",
    ")\n",
    "\n",
    "annotated_frame = annotate(image_source=image_source, boxes=boxes, logits=logits, phrases=[\"object\"])\n",
    "cv2.imwrite(\"annotated_image.jpg\", annotated_frame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "UserWarning: Failed to load custom C++ ops. Running on CPU mode Only!\n"
     ]
    }
   ],
   "source": [
    "from typing import Optional\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torchvision\n",
    "from groundingdino.util.inference import annotate, batch_predict, load_image, load_model\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.Resize([800, 1000]),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "def create_dataloader(\n",
    "    data_dir: str,\n",
    "    batch_size: int = 2,\n",
    "    shuffle: bool = True,\n",
    "    num_workers: int = 8,\n",
    "    transform: Optional[torchvision.transforms.Compose] = None,\n",
    ") -> DataLoader:\n",
    "    \"\"\"\n",
    "    Creates a PyTorch DataLoader for images stored in subdirectories.\n",
    "\n",
    "    Parameters:\n",
    "    - data_dir (str): Path to the main directory containing subdirectories of images.\n",
    "    - batch_size (int): Number of samples per batch to load.\n",
    "    - shuffle (bool): Whether to shuffle the dataset.\n",
    "    - num_workers (int): How many subprocesses to use for data loading.\n",
    "    - transform (torchvision.transforms.Compose): Transformations to apply to the images.\n",
    "\n",
    "    Returns:\n",
    "    - DataLoader: PyTorch DataLoader.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load the dataset from the directory with subdirectories\n",
    "    dataset = datasets.ImageFolder(root=data_dir, transform=transform)\n",
    "\n",
    "    # Create the DataLoader\n",
    "    dataloader = DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=shuffle, num_workers=num_workers\n",
    "    )\n",
    "\n",
    "    return dataloader\n",
    "\n",
    "\n",
    "image_data_dir = \"/Users/derek/Desktop/Drone-detection-dataset/output_frames\"\n",
    "dataloader = create_dataloader(data_dir=image_data_dir, transform=transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3588.)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final text_encoder_type: bert-base-uncased\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/590 [00:00<?, ?it/s]FutureWarning: The `device` argument is deprecated and will be removed in v5 of Transformers.\n",
      "UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.4 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "UserWarning: None of the inputs have requires_grad=True. Gradients will be None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.5485951900482178\n",
      "self_attn time: 0.6338047981262207\n",
      "Pytorch Time:  0.5146548748016357\n",
      "self_attn time: 0.6318199634552002\n",
      "Pytorch Time:  0.5105071067810059\n",
      "self_attn time: 0.5999727249145508\n",
      "Pytorch Time:  0.5168209075927734\n",
      "self_attn time: 0.6073100566864014\n",
      "Pytorch Time:  0.4995300769805908\n",
      "self_attn time: 0.5888190269470215\n",
      "Pytorch Time:  0.49999213218688965\n",
      "self_attn time: 0.5902009010314941\n",
      "fusion time: 0.08512282371520996, text time: 0.043663740158081055, transformer time: 3.6624248027801514\n",
      "Pytorch Time:  0.06615400314331055\n",
      "Pytorch Time:  0.049056053161621094\n",
      "Pytorch Time:  0.05219888687133789\n",
      "Pytorch Time:  0.04783010482788086\n",
      "Pytorch Time:  0.05721306800842285\n",
      "Pytorch Time:  0.05754709243774414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/590 [00:07<1:13:36,  7.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.5252058506011963\n",
      "self_attn time: 0.5963060855865479\n",
      "Pytorch Time:  0.5177521705627441\n",
      "self_attn time: 0.6411271095275879\n",
      "Pytorch Time:  0.5075130462646484\n",
      "self_attn time: 0.6029500961303711\n",
      "Pytorch Time:  0.5106492042541504\n",
      "self_attn time: 0.6097381114959717\n",
      "Pytorch Time:  0.5069680213928223\n",
      "self_attn time: 0.6060240268707275\n",
      "Pytorch Time:  0.5084800720214844\n",
      "self_attn time: 0.6019949913024902\n",
      "fusion time: 0.011992692947387695, text time: 0.006629467010498047, transformer time: 3.6610846519470215\n",
      "Pytorch Time:  0.048905134201049805\n",
      "Pytorch Time:  0.051896095275878906\n",
      "Pytorch Time:  0.05195498466491699\n",
      "Pytorch Time:  0.05075693130493164\n",
      "Pytorch Time:  0.051338911056518555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 2/590 [00:11<55:15,  5.64s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.053604841232299805\n",
      "Pytorch Time:  0.5213799476623535\n",
      "self_attn time: 0.5947558879852295\n",
      "Pytorch Time:  0.5274498462677002\n",
      "self_attn time: 0.6455888748168945\n",
      "Pytorch Time:  0.5264501571655273\n",
      "self_attn time: 0.6238491535186768\n",
      "Pytorch Time:  0.5283050537109375\n",
      "self_attn time: 0.6273159980773926\n",
      "Pytorch Time:  0.5316920280456543\n",
      "self_attn time: 0.6287310123443604\n",
      "Pytorch Time:  0.5271720886230469\n",
      "self_attn time: 0.6243329048156738\n",
      "fusion time: 0.012691020965576172, text time: 0.008030176162719727, transformer time: 3.7481775283813477\n",
      "Pytorch Time:  0.04676413536071777\n",
      "Pytorch Time:  0.05258607864379883\n",
      "Pytorch Time:  0.04689478874206543\n",
      "Pytorch Time:  0.05361318588256836\n",
      "Pytorch Time:  0.052063941955566406\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 3/590 [00:16<49:35,  5.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.05165576934814453\n",
      "Pytorch Time:  0.5251140594482422\n",
      "self_attn time: 0.5988938808441162\n",
      "Pytorch Time:  0.5103049278259277\n",
      "self_attn time: 0.6089341640472412\n",
      "Pytorch Time:  0.5152387619018555\n",
      "self_attn time: 0.6147069931030273\n",
      "Pytorch Time:  0.5123729705810547\n",
      "self_attn time: 0.6110589504241943\n",
      "Pytorch Time:  0.5128920078277588\n",
      "self_attn time: 0.6090822219848633\n",
      "Pytorch Time:  0.5124959945678711\n",
      "self_attn time: 0.6076319217681885\n",
      "fusion time: 0.013339042663574219, text time: 0.006946086883544922, transformer time: 3.6535825729370117\n",
      "Pytorch Time:  0.05305290222167969\n",
      "Pytorch Time:  0.05506134033203125\n",
      "Pytorch Time:  0.048835039138793945\n",
      "Pytorch Time:  0.05861186981201172\n",
      "Pytorch Time:  0.05795907974243164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 4/590 [00:20<46:39,  4.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.053518056869506836\n",
      "Pytorch Time:  0.51251220703125\n",
      "self_attn time: 0.586345911026001\n",
      "Pytorch Time:  0.5141799449920654\n",
      "self_attn time: 0.6281590461730957\n",
      "Pytorch Time:  0.5113070011138916\n",
      "self_attn time: 0.6058602333068848\n",
      "Pytorch Time:  0.5235280990600586\n",
      "self_attn time: 0.6189968585968018\n",
      "Pytorch Time:  0.5218400955200195\n",
      "self_attn time: 0.6276500225067139\n",
      "Pytorch Time:  0.5109720230102539\n",
      "self_attn time: 0.6063940525054932\n",
      "fusion time: 0.012588262557983398, text time: 0.007185220718383789, transformer time: 3.6765754222869873\n",
      "Pytorch Time:  0.05623292922973633\n",
      "Pytorch Time:  0.057057857513427734\n",
      "Pytorch Time:  0.05174112319946289\n",
      "Pytorch Time:  0.053668975830078125\n",
      "Pytorch Time:  0.05512094497680664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 5/590 [00:24<45:07,  4.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.057624101638793945\n",
      "Pytorch Time:  0.5115437507629395\n",
      "self_attn time: 0.5860848426818848\n",
      "Pytorch Time:  0.5120439529418945\n",
      "self_attn time: 0.6211440563201904\n",
      "Pytorch Time:  0.5223448276519775\n",
      "self_attn time: 0.6173350811004639\n",
      "Pytorch Time:  0.5100417137145996\n",
      "self_attn time: 0.6053140163421631\n",
      "Pytorch Time:  0.5097939968109131\n",
      "self_attn time: 0.60416579246521\n",
      "Pytorch Time:  0.5241611003875732\n",
      "self_attn time: 0.6184799671173096\n",
      "fusion time: 0.011948347091674805, text time: 0.0067996978759765625, transformer time: 3.65539813041687\n",
      "Pytorch Time:  0.058432817459106445\n",
      "Pytorch Time:  0.05436205863952637\n",
      "Pytorch Time:  0.05371403694152832\n",
      "Pytorch Time:  0.056369781494140625\n",
      "Pytorch Time:  0.05846381187438965\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 6/590 [00:29<44:07,  4.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.05336713790893555\n",
      "Pytorch Time:  0.5221760272979736\n",
      "self_attn time: 0.5968530178070068\n",
      "Pytorch Time:  0.531329870223999\n",
      "self_attn time: 0.6515259742736816\n",
      "Pytorch Time:  0.5202209949493408\n",
      "self_attn time: 0.6137049198150635\n",
      "Pytorch Time:  0.5179681777954102\n",
      "self_attn time: 0.6210939884185791\n",
      "Pytorch Time:  0.5253820419311523\n",
      "self_attn time: 0.6229958534240723\n",
      "Pytorch Time:  0.5247070789337158\n",
      "self_attn time: 0.6220898628234863\n",
      "fusion time: 0.012556314468383789, text time: 0.0069735050201416016, transformer time: 3.7314279079437256\n",
      "Pytorch Time:  0.0547940731048584\n",
      "Pytorch Time:  0.05631589889526367\n",
      "Pytorch Time:  0.0529630184173584\n",
      "Pytorch Time:  0.058220863342285156\n",
      "Pytorch Time:  0.057324886322021484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/590 [00:33<43:42,  4.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.052490949630737305\n",
      "Pytorch Time:  0.5351569652557373\n",
      "self_attn time: 0.6105780601501465\n",
      "Pytorch Time:  0.5127489566802979\n",
      "self_attn time: 0.6363050937652588\n",
      "Pytorch Time:  0.5172600746154785\n",
      "self_attn time: 0.6132547855377197\n",
      "Pytorch Time:  0.5064640045166016\n",
      "self_attn time: 0.602520227432251\n",
      "Pytorch Time:  0.5101110935211182\n",
      "self_attn time: 0.6036202907562256\n",
      "Pytorch Time:  0.5076930522918701\n",
      "self_attn time: 0.602083683013916\n",
      "fusion time: 0.012119531631469727, text time: 0.00678253173828125, transformer time: 3.6713879108428955\n",
      "Pytorch Time:  0.05077505111694336\n",
      "Pytorch Time:  0.04752516746520996\n",
      "Pytorch Time:  0.04871821403503418\n",
      "Pytorch Time:  0.05303502082824707\n",
      "Pytorch Time:  0.04900503158569336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|▏         | 8/590 [00:38<43:07,  4.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.04913830757141113\n",
      "Pytorch Time:  0.5060710906982422\n",
      "self_attn time: 0.5792431831359863\n",
      "Pytorch Time:  0.5116159915924072\n",
      "self_attn time: 0.6295380592346191\n",
      "Pytorch Time:  0.5208392143249512\n",
      "self_attn time: 0.6175780296325684\n",
      "Pytorch Time:  0.5236780643463135\n",
      "self_attn time: 0.6197528839111328\n",
      "Pytorch Time:  0.515434980392456\n",
      "self_attn time: 0.6093549728393555\n",
      "Pytorch Time:  0.5336000919342041\n",
      "self_attn time: 0.6288411617279053\n",
      "fusion time: 0.011993169784545898, text time: 0.0068988800048828125, transformer time: 3.687317132949829\n",
      "Pytorch Time:  0.05352306365966797\n",
      "Pytorch Time:  0.05176496505737305\n",
      "Pytorch Time:  0.0524601936340332\n",
      "Pytorch Time:  0.05241799354553223\n",
      "Pytorch Time:  0.054984092712402344\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 9/590 [00:42<42:49,  4.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.055248260498046875\n",
      "Pytorch Time:  0.5102789402008057\n",
      "self_attn time: 0.5858008861541748\n",
      "Pytorch Time:  0.5184369087219238\n",
      "self_attn time: 0.6360511779785156\n",
      "Pytorch Time:  0.5088157653808594\n",
      "self_attn time: 0.603208065032959\n",
      "Pytorch Time:  0.5152640342712402\n",
      "self_attn time: 0.6111340522766113\n",
      "Pytorch Time:  0.5061759948730469\n",
      "self_attn time: 0.600862979888916\n",
      "Pytorch Time:  0.5063669681549072\n",
      "self_attn time: 0.6009211540222168\n",
      "fusion time: 0.012401580810546875, text time: 0.0070607662200927734, transformer time: 3.6409976482391357\n",
      "Pytorch Time:  0.05237579345703125\n",
      "Pytorch Time:  0.053156137466430664\n",
      "Pytorch Time:  0.04714798927307129\n",
      "Pytorch Time:  0.056694984436035156\n",
      "Pytorch Time:  0.04557490348815918\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/590 [00:46<42:25,  4.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pytorch Time:  0.04699397087097168\n",
      "Pytorch Time:  0.5201008319854736\n",
      "self_attn time: 0.5952560901641846\n",
      "Pytorch Time:  0.5050821304321289\n",
      "self_attn time: 0.6286211013793945\n",
      "Pytorch Time:  0.5221009254455566\n",
      "self_attn time: 0.6190617084503174\n",
      "Pytorch Time:  0.5262589454650879\n",
      "self_attn time: 0.6220660209655762\n",
      "Pytorch Time:  0.5062999725341797\n",
      "self_attn time: 0.6005418300628662\n",
      "Pytorch Time:  0.5055360794067383\n",
      "self_attn time: 0.5992171764373779\n",
      "fusion time: 0.01211237907409668, text time: 0.006850004196166992, transformer time: 3.667639970779419\n",
      "Pytorch Time:  0.055577754974365234\n",
      "Pytorch Time:  0.05338788032531738\n",
      "Pytorch Time:  0.0485990047454834\n",
      "Pytorch Time:  0.05106306076049805\n",
      "Pytorch Time:  0.05702495574951172\n",
      "Pytorch Time:  0.05287981033325195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 10/590 [01:31<1:28:02,  9.11s/it]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.profiler import ProfilerActivity, record_function, profile\n",
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, batch_predict\n",
    "import tqdm\n",
    "\n",
    "TEXT_PROMPT = \"flying object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "device = \"mps\"\n",
    "\n",
    "# Load the model\n",
    "model = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\").to(device)\n",
    "\n",
    "# Define your dataloader here\n",
    "# dataloader = ...\n",
    "\n",
    "for idx, batch in enumerate(tqdm.tqdm(dataloader)):\n",
    "    boxes, logits, boxes_to_im = batch_predict(\n",
    "        model=model,\n",
    "        preprocessed_images=batch[0],\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=device\n",
    "    )\n",
    "    # Step to advance the profiler\n",
    "    if idx == 10: \n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from groundingdino.util.inference import load_model, load_image, predict, annotate, batch_predict\n",
    "import tqdm\n",
    "TEXT_PROMPT = \"flying object\"\n",
    "BOX_TRESHOLD = 0.35\n",
    "TEXT_TRESHOLD = 0.25\n",
    "model = load_model(\"../GroundingDINO/groundingdino/config/GroundingDINO_SwinT_OGC.py\", \"groundingdino_swint_ogc.pth\").to(\"mps\")\n",
    "\n",
    "for batch in tqdm.tqdm(dataloader):\n",
    "    boxes, logits, boxes_to_im = batch_predict(\n",
    "        model=model,\n",
    "        preprocessed_images=batch[0],\n",
    "        caption=TEXT_PROMPT,\n",
    "        box_threshold=BOX_TRESHOLD,\n",
    "        text_threshold=TEXT_TRESHOLD,\n",
    "        device=\"mps\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
